---
layout: post
title: 音视频开发进阶指南笔记（1）
tags:
  - 音视频
---


# 音视频开发进阶指南笔记（1）

## 一、基础概念

### 1.1 声音的物理性质

#### 1.1.1 声音是波

#### 1.1.2 声波的三要素

* 频率： 频率高，波长短，长波更容易绕过障碍物，可以传递的更远
* 响度： 就是声音大小
* 音色： 波形代表了声音的音色

人耳的听力范围 是20 - 20k Hz

![等响曲线](https://zhouxiaofei-image.oss-cn-hangzhou.aliyuncs.com/images/20190613111028.png)

人耳堆3 - 4 Hz 的范围内的声音比较敏感

#### 1.1.3 声音的传播介质

吸音棉，隔音棉

#### 1.1.4 回声

就是声音的反弹

#### 1.1.5 共鸣

声音的传播也是一种能量的传播过程  

### 1.2 数字音频

数字音频有三个概念

* 采样： 就是在时间轴上堆信号进行数字化，根据耐奎斯特定理（采样定理），按照比声音最高频率两倍以上的频率对声音进行采样（也成为AD转换）
* 量化：在幅度轴胜对信号进行数字化，比如用16比特的二进制信号表示声音的一个采样
* 编码： 按照一定的格式记录采样和量化后的数字数据，比如顺序存储或压缩存储等    

通常所说的裸数据格式就是脉冲编码调制（PCM）数据。 描述一段PCM数据一般需要以下几个概念： *量化格式，采样率，声道数。 以CD音质为例： 量化格式为16比特，采样率为44100，声道数为2   

对于声音格式，还有一个概念来描述它的大小，成为数据比特率，即一秒时间内的比特数目。用于衡量音频数据单位时间内的容量大小， 对于CD音质来说，比特率就是 44100 * 16 * 2 kps     

在一分钟的时间内，这类CD音质的数据需要占据的空间： 44100 * 16 * 2 / 1000 * 60 / 8 / 1024 M  

如果sampleFormat更精确，或者 sampleRate更密集， 那么所占的空间就会更大，同时描述的声音细节就会更精确。   

#### 麦克风是如何采集数据的

麦克风里面有一层碳膜，非常薄且十分灵敏，声波会压缩空气，进而压缩这层碳膜，碳膜在受到挤压的时候也会发出震动，在碳膜的下方就是一个电极，碳膜在震动的时候会接触电极，接触时间长短和频率与声波的震动幅度和频率有关，这样就玩成了声音信号到电信号的转换。之后再经过放大电路的处理，就可以实施采样量化处理了。   


### 1.3 音频编码   

网络传播中需要对声音进行压缩编码，压缩编码的基本指标之一就是压缩比，通常小于1（废话）。   

压缩算法有__有损压缩__ 和 __无损压缩__， 无损压缩是指解压后的数据可以完全复原。  有损压缩反之，常用的格式中，无损压缩用的较多。   根据不同需求，采用不同的压缩编码算法。  如，PCM、WAV、AAC、MP3、Ogg等    

压缩编码的原理实际上是压缩掉冗余信号（不能被人耳感知的信号），包含听觉范围之外的音频信号以及被掩盖掉的音频信号   

#### 常用的编码格式

* WAV： 一种实现，就是在PCM数据格式前面加上44 字节，分别用来描述 PCM的采样率，声道数，数据格式等信息   特点： 音质好，适配范围广
* MP3编码： MP3具有不错的压缩比， 特点： 音质在 128 Kbit/s以上表现不错，压缩比较高
* AAC编码： 通过一些附加的编码技术（PS，SBR 等），衍生出来 LC—AAC，HE—AAC，HE-AAC v2 三种主要的编码格式。分别对应 高码率（>80Kbit/s），中码率，和低码率（<48Kbit/s），特点： 低码率下表现优异，并且多用于视频中的音频解码。
* Ogg编码： 在各个码率下都有优异的表现，尤其是中低码率下。。 但是目前还没有媒体服务软件的支持 ， 特点： 表现优异，但是兼容性不够好，流媒体特性不支持。。使用与语音聊天的音频消息场景 

### 1.4 图像的物理现象  

视频是由一幅幅图像组成的，所以要学习视频还得从学习图像开始   

看到物体是因为光的反射   

### 1.5 图像的数值表示  

#### 1.5.1 RGB 的表示方式   

像素里面的子像素如何表示 ？ 

* 浮点表示： 取值范围为0.0 - 1.0 ，openGL ES 就是如此
* 整数表示： 取值范围是0-255， 8个比特表示一个子像素，32 个比特表示一个像素（4个颜色通道），有些平台表示一个像素的比特也不相同。。图像也使用压缩格式在网络上传播（jpg，png等），但是对于视频来说，这种压缩不能直接用于视频压缩，因为视频除了考虑帧内编码，也要考虑帧间编码，视频压缩采用的是更成熟的算法。

#### 1.5.2 YUV 表示方式  

对于视频帧的裸数据表示，更多采用的是YUV 数据格式，只要用于优化彩色视频信号的传输相比，最大的有点在于只需要占用极少的带宽， Y （表示明亮度），U和V 代表色度，他们的作用是描述影像的色彩以及饱和度，永来指定像素的颜色。。  

亮度 是透过RGB 输入信号来建立的，方法是将RGB 的特定部分叠加道一起。。   
色度 定义了颜色的两个方面，颜色和饱和度，分别用 Cr 和 Cb表示，其中Cr 表示了RGB 输入信号红色部分与RGB信号亮度值之间的差异，而Cb 表示的是 RGB蓝色部分与RGB信号亮度值之间的差异   

永YUV 的原因是 它的亮度信号Y 和色度信号 U，V 是分离的，    
最常用的表示形式是 Y、U、V都使用8个字节为准，所以取值范围就是0-255， 在广播电视系统中不传输很低和很高的数值，实际上是为了防止信号变动造成过载，所以把两边的数值作为“保护带”， Y 的取值大概在16 - 235， UV 的取值在16 - 240，   

### 1.6 视频的编码方式

#### 1.6.1 视频编码

同样是去除冗余信息：  

* 运动补偿： 通过先前的局部图像来预测、补偿当前的局部图像，从而减少帧序列冗余信息   
* 运动表示： 不同区域的图像需要使用不同的运动矢量来描述运动信息
* 运动估计： 运动估计是从视频序列中抽取运动信息的一整套技术   

使用帧内编码可以去除空间上的冗余信息，Motion JPEG即 MPEG，是适用于动态视频的压缩算法。。除了对单幅图像进行编码外，还利用图像序列中的相关原则除去冗余，这样可以大大提高视频的压缩比， 目前主要有以下几个版本：  Mpeg1（VCD），Mpeg2（DVD）， Mpeg4 AVC（目前流媒体主要使用）   

相比较于 ISO 制定的MPEG 标准，ITU-T 制定的H.261 ,H.262, H.263, H.264 一系列视频编码标准是一套单独的体系，其中，H.264 集中了以往标准的所有优点，并吸取了以往的标准经验。这使得他比 MPeg更好推广，目前使用最多的就是这个。   

H.264 创造了多参考帧，多块类型，整数转换，帧内预测等新的压缩技术，使用了更精细的分像素运动矢量，和新一代的环路滤波器，大大提高了压缩性能。


#### 1.6.2 编码概念

##### 1. IPB 帧

压缩每一帧的数据   
GOP说白了就是两个I帧之间的间隔.比较说GOP为120,如果是720p60的话,那就是2s一次I帧. 

* I frame ：帧内编码帧 又称intra picture，I 帧通常是每个 GOP（MPEG 所使用的一种视频压缩技术）的第一个帧，经过适度地压缩，做为随机访问的参考点，可以当成图象。I帧可以看成是一个图像经过压缩后的产物。

* P frame: 前向预测编码帧 又称predictive-frame，通过充分将低于图像序列中前面已编码帧的时间冗余信息来压缩传输数据量的编码图像，也叫预测帧；

* B frame: 双向预测内插编码帧 又称bi-directional interpolated prediction frame，既考虑与源图像序列前面已编码帧，也顾及源图像序列后面已编码帧之间的时间冗余信息来压缩传输数据量的编码图像，也叫双向预测帧；

* PTS：Presentation Time Stamp。PTS主要用于度量解码后的视频帧什么时候被显示出来

* DTS：Decode Time Stamp。DTS主要是标识读入内存中的ｂｉｔ流在什么时候开始送入解码器中进行解码。

在没有B帧存在的情况下DTS的顺序和PTS的顺序应该是一样的。

### 2.1 创建xcode项目，添加C++支持

### 2.3 交叉编译的原理与实践

#### 2.3.1 交叉编译的原理

交叉编译： 在一个平台上，生成另外一个平台的可执行代码，相较于正常编译，：

* 首先，最终运行程序的设备就是iOS 或安卓设备，源代码就是下载的或者自己编写的源代码，编译机器就是自己的pC，编译器也在PC上面，编译器就是交叉工具编译链   

无论是自行安装的编译器，还是下载其他平台的交叉工具编译链，他们都会提供以下几个工具：  CC， AS， AR， LD， NM， GDB，

* CC： 编译器，对C 源文件进行编译处理
* AS： 将汇编文件生成目标文件（翻译称机器码）
* AR： 打包器，用于库操作，可以通过该工具从一个库中删除或者增加代码模块
* LD： 链接器，为前面生成的目标代码分配地址空间，将多个文件链接称一个库或者是可执行文件
* GDB： 调试工具，可以对运行过程中的代码进行调试
* STRIP： 以最终生成的可执行文件或者库文件作为输入，然后消除掉其中的源码
* NM： 查看静态库文件中的符号表
* OBJdump： 查看静态库或者动态库的方法签名  


##### LAME 的交叉编译

LAME 是一款非常优秀的MP3 编码引擎，在业界是最常用的。如果在移动端编码MP3文件，使用LAME 就成为了唯一的选择。   

iOS 交叉编译工具链是随着Xcode 一起安装的，所以不需要自己单独下载安装。  

下载LAME 的最新版本 [链接地址](https://sourceforge.net/projects/lame/files/lame/3.99/)


书里面的编译代码有问题， 查看[这个链接](https://www.jianshu.com/p/a98ef8a396ea)

##### FDK_AAC 的交叉编译

FDK_AAC 简介： 
	是用来编码和解码AAC 格式音频文件的开源库，安卓系统使用的就是这个，FDK_AAC 几乎支持大部分的Profile， 并且支持CBR和VBR 这两种模式，    
下载[稳定版本](https://sourceforge.net/p/opencore-amr/fdk-aac/ci/v0.1.4/tree/)

同上，编译代码查看[这里](https://blog.csdn.net/zhjw1991/article/details/80406104)

##### x264 交叉编译
x264是一种免费的、具有更优秀算法的符合H.264/MPEG-4 AVC视频压缩编码标准格式的编码库   
主要看这个： [地址](https://depthlove.github.io/2015/09/16/build-X264-library-for-iOS-platform/)

## 3. FFmpeg

开源框架，可以用来记录、处理、数字音视频，并转换为流的开源框架，采用LPL或GPL 许可证，提供了录制，转换以及流化音视频的完整解决方案，跨平台特性非常强大，FF 是fast forward的缩写    

### 3.1 编译

#### 1. 编译选项详解

* 下载FFmpeg
* FFmpeg 与大部分GNU 软件的编译方式类似，都是通过configure脚本来实现编译前定制的。
* configure脚本运行完毕之后，会生成config.mk 和config.h 两个文件，分别作用道makefile和源代码的层次，由这两个部分协同实现对编译选项的控制。
	* 标准选项： CNU 软件例行配置，例如安装路径， --prefix等
	* 编译、链接选项： 默认配置是生成静态库，例如 -disable-static， --enable-shared等
	* 可执行程序控制选项，决定是否生成FFmpeg，ffplay，ffprobe和ffserver等
	* 模块控制选项： 裁剪编译模块，包括整个库的裁剪，例如--disable-avdevice， 一组模块的筛选，例如--disable-decoders； 单个模块的裁剪， 例如disable-demuxer
	* 能力展示选项： 列出当前源码支持的各种能力集，例如--list-decoders
	* 其他： 允许开发者深度定制，例如交叉编译环境配置，自定以编译器参数的设定等等

下图是整体结构

![FFmpeg结构图](https://zhouxiaofei-image.oss-cn-hangzhou.aliyuncs.com/images/20190613175223.png)



默认的编译会生称4个可执行文件和8个静态库，可执行文件包括： 

* 用于转码、推流、Dump媒体文件的ffmpeg
* 用于播放媒体文件的ffplay
* 用于获取媒体文件信息的ffprobe
* 以及作为简单流媒体服务器的ffserver

8个静态库其实就是FFmpeg 的8个模块

* AVUtil： 核心工具库，基础模块库，其他模块几乎都会依赖
* AVFormat 文件格式和协议库，最重要的模块之一，封装了protocol曾和Demuxer， Muxer层，使协议和格式对于开发者来说是透明的

	> muxer是指合并文件，即将视频文件、音频文件和字幕文件合并为某一个视频格式。比如把rmvb格式的视频，mp3格式的音频文件以及srt格式的字幕文件，合并成为一个新的mp4或者mkv格式的文件。
	
* AVCodec: 编码解码库，也是重要模块之一，封装了Codec层，但是有一些Codec是具备自己的License的，FFmpeg是不会默认添加 libx264， FDK-AAC 等库的，但是FFmpeg就像一个平台，可以将其他的第三方Codec 以插件的形式添加进来，然后为开发者提供统一的接口。
* AVFilter： 音视频滤镜库，提供了包括音频特效和视频特效的处理，在使用FFmpeg 的API进行边解码的过程中，直接使用这个模块为音视频数据做特效处理，非常方便和高校
* AVDevice： 输入输出设别库， 比如，需要编译出播放声音或者播放视频的工具ffplay， 就需要确保该模块是打开的，，同事也需要libSDL的预先编译，因为该设备模块播放声音与播放视频使用的都是libSDL库  
* SwrRessample： 可以用于音频重采样，可以对数字音频进行声道数、数据格式、采样率等多种基本信息的转换。 
* SWScale： 将图像进行格式转化的模块，比如，可以将YUV 的数据转换为RGB 的数据
* PostProc： 可以用于后期树立，当我门使用AVFilter 的时候，需要打开这个模块的开关，因为Filter模块会用到这个模块的基础函数  



### 4. FFmpeg 使用

#### ffprobe 查看文件音视频文件信息

``ffprobe 参数 文件路径``

``ffprobe -show_format 文件`` 输出格式信息 format_name， 时间长度，文件大小，比特率，流的数目等    

``-print_format 目标文件 -show_streams 源文件``  以json格式输出每一个具体的流的详细信息，会有视频宽高、是否有b帧、视频帧的总数量、视频编码格式、显示比例、比特率等信息    

``-show_frames`` 显示帧信息，打印出每一帧的信息   

``-show_packets`` 显示包信息

#### ffplay 播放

``ffplay 文件路径 参数``


#### ffplay 播放

``ffplay name`` 直接播放文件

上下左右按键分别对应快进10秒，快退10秒，快进一分钟，快退一分钟，播放音频的时候按w 会显示波形图   

``ffplay name -loop 10``  代表循环10次播放    

``ffplay name -ast 1``  表示播放视频中的第一路音频流，由最后的数字控制   
``ffplay name -f s16le -channels 2 -ar 44100`` 设置前面这几个参数，就可以正常播放PCM 文件了， 前提是格式 ``-f``， 声道数 ``channels``, 采样率 ``-ar`` 设置正确，如果有一项设置不正确，就不会播放成功。  

WAV 其实就是PCM 头部添加44个字节，表示这个PCM 的各项参数信息，所以可以直接播放WAV 格式的文件  

##### 一帧视频的播放：   

``$ ffplay -f rawvideo -pixel_format yuv420 -s 480*480 texture.yuv``  其实对一帧视频帧，（也就是一张图片），直接可以用ffplay 播放，png或者jpg 会在头部信息里面说明这张图片的宽高和格式，如果想让ffplay 展示一张YUV 格式的原始数据表示的图片，那么需要告诉ffplay一些重要的信息， 包括：  

* 格式 `-f` ``rawvideo`` 代表原始格式
* 表示格式 `-pixel_format yuv420p`
* 宽高 `-s 480*480` 对于RGB 表示的图像，其实是一样的  

```bash
$ ffplay -f rawvideo -pixel_format rgb24 -s 480*480 texture.rgb

```
对于视频播放器，还要注意音画同步的问题，ffplay 音画同步方法有三种： 

* 以音频为主时间轴作为同步源
* 以外部时钟为主时间轴
* 以视频为主时间轴

后面主要以音频为主时间轴    

播放器接收到的视频帧或者音频帧，内部都会有时间戳（PTS 时钟）来标识他应该再什么时间展示，实际的对齐策略如下： 

比较视频的当前播放时间和音频当前的播放时间，如果视频播放较快，则通过加大延迟或者重复播放来降低视频播放速度；如果视频播放慢了，则通过减小延迟或者丢帧来追赶音频的播放时间点。关键点在于时间的比较以及延迟的计算，再比较的过程中会设置一个阈值（Threshold），如果超过阈值就应该调整。   

对于ffplay 可以指定使用那一种对齐方式  

``ffplay name.mp4 -sync audio``  音频

``ffplay name.mp4 -sync video``  视频

``ffplay name.mp4 -sync ext``    外部时钟

#### ffmpeg

ffmpeg 是这三个工具里面最为强大的一个，富媒体文件转换工具，他可以转换任何格式的媒体文件，也可以用自己的AudioFilter 以及 VideoFilter 进行处理和编辑。

##### 通用参数

* -f fmt:  指定格式（音频或者视频格式）
* -i filename: 指定输入文件名，再linux 下可以指定： 0.0 屏幕录制或摄像头
* -y： 覆盖已有文件
* -t duration： 指定时长
* -fs limit_size： 设置文件大小上限
* -ss time_off：从指定时间开始， 也支持 -hh:mm:ss.xxx的格式
* -re : 代表按照帧率发送，尤其再作为推流工具的时候一定要加入该参数，否则ffmpeg会按照最高速率向流媒体服务器不停的发送数据
* -map： 指定输出文件的流映射关系。 例如 ``-map 1: 0-map 1:1`` 要求将第二个输入文件的第一个流和第二个流写入出入文件。如果没有-map选项，则采用默认的映射关系

##### 视频参数

* -b： 指定比特率， ffmpeg 是自动使用VBR 的，指定了该参数则使用平均比特率（bit/s）

	> VBR（Variable Bit Rate）动态比特率。也就是非固定的比特率，音频编码软件在编码时根据音频数据的复杂程度即时确定使用什么比特率，这是以质量为前提兼顾文件大小的编码方式。
* -bitexact： 使用标准比特率
* -vb： 指定视频比特率（bits/s）
* -r rate： 帧速率  （fps）
* -s size： 指定分辨率
* -aspect aspect： 设置视频长宽比 （16:9）
* -croptop size： 设置顶部切除尺寸 （in pixels）
* -cropbottom size： 设置底部切除尺寸
* -cropleft size： 左侧切除尺寸
* -cropright size： 右侧切除尺寸
* -padtop size： 顶部补齐尺寸
* -padbottom size： 底部补齐
* -padleft size： 左侧
* -padright size： 右测
* padcolor color： 补齐带颜色
* -vn： 取消视频输出
* -vcodec codec： 强制使用codec边解码方式，

##### 音频参数

* -ab： 设置比特率（bit/s），MP3格式要听到较高品质的声音建议160kbit/s以上
* -aq quality： 设置音频质量
* -ar rate： 采样率 Hz
* -ac channels： 声道数
* -an： 取消音频轨
* -acodec codec： 指定音频编码
* -vol volume： 设置录制音量大小（默认256）

以下是示例：  

###### 1. 列出所有支持的格式

``ffmpeg -formats``
###### 2. 剪切一段媒体文件
``ffmpeg -i input.mp4 -ss 00:00:50.0 -codec copy -t 20 output.mp4``  -t指定时长

###### 3. 可以切割称多个文件

``ffmpeg -i input.mp4 -t 00:00:00.0 -c copy small-1.mp4 -ss 00:00:30.0 -codec copy small-2.mp4``

###### 4. 提取一个视频文件中的音频文件

``ffmpeg -i input.mp4 -vn -acodec  copy output.m4a``  vn取消视频输出

###### 5.使一个视频中的音频静音，只保留视频

``ffmpeg -i input.mp4 -an -acodec  copy output.m4a``  an取消音频输出

###### 6. 从mp4文件中抽出视频流到处为裸H264 数据

``ffmpeg -i output.mp4 -an -vcodec copy -bsf:v h264_mp4toannexb output.h264`` 不适用音频数据 ``an`` 时评市局使用mp4toannexb这个bitstream filter来转换为原始的h264数据

###### 7. 使用AAC音频数据和H264时评生成mp4文件  

```
ffmpeg -i test.aac -i test.h264 -acodec copy -bsf:a aac_adtstoase -vcode copy -f mp4 output.mp4

```

上述代码使用了一个aac_adtstoasc 的bitstream filter ，AAC 格式也有两种封装格式

###### 8。对音频文件的编码格式转换

```
ffmpeg -i input.wav -acodec libfdk_aac output.aac
```

###### 9. 从WAV音频文件到处PCM裸数据

```
ffmpeg -i input.wav -acodec pc_s16le -f s16le output.pcm
```
这样就可以到处用16bit来表示的一个sample的pcm数据了，并且每个sample的字节排序都是小尾端的表示格式，声道数和采样率使用的都是原始WAV文件的声道数和采样率的PCM数据   

###### 10. 重新编码视频文件，复制音频流，同时封装道mp4的文件中

```
ffmpeg -i input.flv -vcodec libx264 -acodec copy output.mp4
```

###### 11. 将一个mp4格式的视频文件转换称gif 动图

```
ffmpeg -i input.mp4 -vf scale=100:-1 -t 5 -r 10 image.gif
```

上述代码按照比例不动宽度改为100（使用video 的scaleFilter），帧率改为10（-r），只处理前5秒钟的视频（-t）

###### 12. 将一个视频离得画面全部生成图片

```
ffmpeg -i output.mp4 -r 0.25 frames_%4d.png
```

每4秒钟截取一张图片

###### 13. 使用一组图片可以组成一个gif， 如果连拍了一组图片，也可以组成gif

```
ffmpeg -i frames_$04d.png pr 5 output.gif
```

###### 14. 使用音量效果器，可以改变一个音频媒体文件中的音量

```
ffmpeg -i input.mp4 -af 'volume=0.5' output.wav
```

声音减小一半

###### 15 淡入效果器的使用

```
ffmpeg -i inpput.wav -filter_complex afade=t=in:ss=0:d=5 output.wav
```

上述命令可以将输入文件的前5s做一个淡入效果，输出道output中，可以将处理之前和处理之后的文件查看波形图

###### 16. 淡出效果器

```
ffmpeg -i input.wav -filter_complex afade=t=out:st=200:d=5 output.wav
```

从200秒开始，做5s的淡出效果

###### 17. 将两路声音合并，比如要给一段声音加上背景音乐： 

```
ffmpeg -i vocal.wav -i accompany.wav -filter_complex amix=inputs=2:duration=shortest output.wav
```

按照时间长度较短的音频文件的时间长度作为最终输出的音频文件长度

###### 18. 对声音进行变速但是不变调

```
ffmpeg -i vocal.wav -filter_complex atempo=0.5 output.wav
```
按照0.5倍速进行处理，时间长度会变为输入的2倍但是音调高低不变

###### 19. 为视频添加水印

```
ffmpeg -i input.mp4 -i changba_icon.png -filter_complex '[0:v][1:v]overlay=main_w-overlay_w-10:10:1[out]' -map '[out]' output.mp4
```
main_W 表示主视频宽度， overlay_w表示水印跨度，main_h代表主视频高度，overlay_h 代表水印高度

###### 20. 视频提亮效果器

```
ffmpeg -i input.flv -c:v libx264 -b:v 880k -c:a libfdk_aac -vf eq=brightnes=0.25 -f mp4 output.mp4 
```

提亮参数是brightness， 取值范围是-1.0 到 1.0.默认是0

###### 21 为视频增加对比度效果
```
ffmpeg -i inpuut -c:v libx264 -b:v 800k libfdk_aax -vfeq=contrast=1.5 -f mp4 output.mp4
```

###### 22 视频旋转效果器

```
ffmpeg -i input.mp4 -vf "transpose=1" -b:v 600k output.mp4
```

###### 23 视频裁剪效果器

```
ffmpeg -i input.mp4 -an -vf 'crop=240:480:120:0' -vcodec libx264 -b:v 600k output.mp4
```

###### 24 将一张RGBA 图片转换为 JPG格式

```
ffmpeg -f rawvideo -pix_fmt rgba s 480*480 -i texture.rgb -f image2 -vcodec mjpeg output.jpg
```

###### 25 将一个YUV 格式数据转换为JPEG格式图片


```
ffmpeg -f rawvideo -pix_fmt yuv420p -s 480*480 -i texture.yuv -f image2 -vcodec mjpeg
        output.jpg

```

###### 26 一段视频推送到流媒体服务器上

```
ffmpeg -re -i input.mp4 -acodec copy -vcodec copy -f flv rtmp://xxx
```

###### 27 将流媒体服务器的流dump到本地

```
ffmpeg -i http://xxx/xxx/xxx.flv -acodec copy -vcodec copy -f flv output.flv
```

###### 28 将两个音频文件以两路流的形式封装到一个文件中，比如再K歌的场景中，原唱伴唱实时切换的场景，可以使用一个文件包含两路流，一路是伴奏，一路是原唱： 

```
ffmpeg -i 111.mp3 -i 222.mp3 -map 0:a -c:a:0 libfdk_aac -b:a:0 96k -map 1:a -c:a:1 libfdk_aac -b:a:1 64k -vn -f mp4 output.m4a
```

ffmpeg有很多很强大的功能，随意组合会有很多牛逼的效果。

### 3.2 FFmpeg API 的介绍与使用

* 容器/文件（container/File）： 即特定格式的多媒体文件
* 媒体流（Stream）：表示时间轴上的一段连续数据，如一段声音数据，视频或字幕数据，如果是压缩的要关联特定的编解码器
* 数据帧/数据包（Frame/Packet）：通常，一个媒体流是由大量的数据帧组成的，对于压缩数据，帧对应这编解码器最小的处理单元，分属于不同媒体流的数据帧交错存储于容器之中
* 编解码器： 编解码器是以帧为单位实现压缩数据和原始数据之间的相互转换的

`AVFormatContext` 就是对容器或者媒体文件层次的一个抽象，这个容器中，包含了多路流，对流的抽象就是`AVStream`； 在每一路流中都会描述这路流 的编码格式，对编解码格式以及编解码器的抽象就是`AVCodecContext`与`AVCodec`； 对于编解码器的输入输出部分，也就是压缩数据以及原始数据的抽象就是AVPacket与AVFrame。   
以上就是FFmpeg 的抽象层次，针对与AVFrame的处理，使用的是AVFilter



### 3.3 FFmpeg 的源码结构

#### 3.3.1  libavformat 与libavcodec 

liavformat 层次结构如下

![](https://zhouxiaofei-image.oss-cn-hangzhou.aliyuncs.com/images/20190617113706.png)

AVFormatContext 是API层直接接触到的结构体，进行格式的封装与接封装，数据部分由底层提供，底层使用了AVIOContext， 这个context实际上就是为普通的I/O增加了一层Buffer缓冲区，再往底层就是URLContext，也就是达到了协议层，协议层的具体实现有跟多，包括rtmp，http，hls，file等，这就是libavformat的内部封装了。

libavcodec内部组成

![](https://zhouxiaofei-image.oss-cn-hangzhou.aliyuncs.com/images/20190617114004.png)

对于开发者来说，这一层我们能接触到的最底层的结构体就是AVCodecContext，这个结构体包含的就是与实际的编解码有关地 部分。 首先AVCodecContext 是包含再一个AVStream里面的，即描述了这路流的编码格式是什么，其中存放了具体的编码格式信息，跟忽悠codec的信息可以打开编码器或者解码器，然后利用该解码器或者编码器进行AVPacket与AVFrame 之间的转换，下面看下内部后主要做了什么  


1. av_regist_all 
	
	再编译FFmpeg的时候，configure 配置会生成两个文件： config.mk 与config.h， config.mk实际上就是makefile文件需要包含进去的子模块，会作用再编译阶段，帮助开发者编译出来正确的库，而config.h 是作用再运行阶段，这一阶段将确定需要注册哪些容器以及编码格式到FFmpeg框架中。  
	所以函数内部的实现会先调用avcodec_register_all来注册所有config.h里面开放的编解码器，然后会注册所有的Muxer和Demucer（也就是封装格式），最后注册所有的Protocol，这样依赖，再configure过程中开启或关闭的选项就作用到了运行时。
	
2. av_fing_codec 

	这里面包含了两部分： 寻找编码器和寻找解码器，再第一步的 avcodec_regist_all 函数里面已经把编码器和解码器都存放到一个和链表中了，在这里寻找编码器和解码器都是从第一步构造的链表中便利，通过codec 的 ID 或者name进行条件匹配，

3. avcodec_open2 
	打开编解码器的函数，编解码过程和解码过程都会用到这个函数，输入参数有3个
	
	* AVCodecContext ，解码过程有FFmpeg引擎填充，编码过程由开发者自己构造，如果想要传入私有参数，则为它的priv_data 设置参数，比如再libx264编码器中设置preset，tune，profile等，
	* 第二个参数是上一步通过av_find_codec 寻找出来的编解码器
	* 第三个参数一半会传递NULL
	
	具体到函数的实现时，就会找到对应的实现文件
	
4. avcodec_close 
	
	close 就是上一个函数的逆过程
	
#### 3.3.3 解码时用到的函数

1. avformat_open_input 
	
	根据提供的文件路径判断文件格式，就是根据这个过程决定使用的到底是哪一个Demuxer，  
2. avformat_find_stream_info   
	这个函数比较重要，该方法的作用就是把所有stream的MetaData信息填充好，方法内部会先查找对应的解码器，然后打开对应的解码器，紧接着即用Demuxer中的read_packet函数读取一段数据进行解码，当然解码的数据越多，分析出的流信息就会越准确，如果是本地资源，很快就会得到准确的信息，但是对于网络资源来说则会比较满，因为这个函数有几个参数